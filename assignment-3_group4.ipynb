{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier  # interpret ebm class\n",
    "from interpret.perf import ROC                                # ROC measure for ebm\n",
    "import itertools                                              # for cartesian product of parameters\n",
    "import matplotlib.pyplot as plt                               # for plots\n",
    "import numpy as np                                            # for basic array manipulation                            \n",
    "import pandas as pd                                           # for dataframe manipulation\n",
    "import random                                                 # to sample from lists\n",
    "from sklearn.metrics import accuracy_score, f1_score          # for selecting model cutoffs\n",
    "import time                                                   # for timers\n",
    "\n",
    "# set numpy random seed for better reproducibility\n",
    "SEED = 1988 \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# set number of threads\n",
    "NTHREAD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(frame, y, yhat, by=None, level=None, cutoff=0.5, verbose=True):\n",
    "\n",
    "    \"\"\" Creates confusion matrix from pandas dataframe of y and yhat values, can be sliced \n",
    "        by a variable and level.\n",
    "    \n",
    "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        :param y: Name of actual value column.\n",
    "        :param yhat: Name of predicted value column.\n",
    "        :param by: By variable to slice frame before creating confusion matrix, default None.\n",
    "        :param level: Value of by variable to slice frame before creating confusion matrix, default None.\n",
    "        :param cutoff: Cutoff threshold for confusion matrix, default 0.5. \n",
    "        :param verbose: Whether to print confusion matrix titles, default True. \n",
    "        :return: Confusion matrix as pandas dataframe. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # determine levels of target (y) variable\n",
    "    # sort for consistency\n",
    "    level_list = list(frame[y].unique())\n",
    "    level_list.sort(reverse=True) \n",
    "\n",
    "    # init confusion matrix\n",
    "    cm_frame = pd.DataFrame(columns=['actual: ' +  str(i) for i in level_list], \n",
    "                            index=['predicted: ' + str(i) for i in level_list])\n",
    "    \n",
    "    # don't destroy original data\n",
    "    frame_ = frame.copy(deep=True)\n",
    "    \n",
    "    # convert numeric predictions to binary decisions using cutoff\n",
    "    dname = 'd_' + str(y)\n",
    "    frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
    "    \n",
    "    # slice frame\n",
    "    if (by is not None) & (level is not None):\n",
    "        frame_ = frame_[frame[by] == level]\n",
    "    \n",
    "    # calculate size of each confusion matrix value\n",
    "    for i, lev_i in enumerate(level_list):\n",
    "        for j, lev_j in enumerate(level_list):\n",
    "            cm_frame.iat[j, i] = frame_[(frame_[y] == lev_i) & (frame_[dname] == lev_j)].shape[0]\n",
    "            # i, j vs. j, i nasty little bug ... updated 8/30/19\n",
    "    \n",
    "    # output results\n",
    "    if verbose:\n",
    "        if by is None:\n",
    "            print('Confusion matrix:')\n",
    "        else:\n",
    "            print('Confusion matrix by ' + by + '=' + str(level))\n",
    "    \n",
    "    return cm_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function to calculate AIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def air(cm_dict, reference_key, protected_key, verbose=True):\n",
    "\n",
    "    \"\"\" Calculates the adverse impact ratio as a quotient between protected and \n",
    "        reference group acceptance rates: protected_prop/reference_prop. \n",
    "        Optionally prints intermediate values. ASSUMES 0 IS \"POSITIVE\" OUTCOME!\n",
    "\n",
    "        :param cm_dict: Dictionary of demographic group confusion matrices. \n",
    "        :param reference_key: Name of reference group in cm_dict as a string.\n",
    "        :param protected_key: Name of protected group in cm_dict as a string.\n",
    "        :param verbose: Whether to print intermediate acceptance rates, default True. \n",
    "        :return: AIR.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-20 # numeric stability and divide by 0 protection\n",
    "    \n",
    "    # reference group summary\n",
    "    reference_accepted = float(cm_dict[reference_key].iat[1,0] + cm_dict[reference_key].iat[1,1]) # predicted 0's\n",
    "    reference_total = float(cm_dict[reference_key].sum().sum())\n",
    "    reference_prop = reference_accepted/reference_total\n",
    "    if verbose:\n",
    "        print(reference_key.title() + ' proportion accepted: %.3f' % reference_prop)\n",
    "    \n",
    "    # protected group summary\n",
    "    protected_accepted = float(cm_dict[protected_key].iat[1,0] + cm_dict[protected_key].iat[1,1]) # predicted 0's\n",
    "    protected_total = float(cm_dict[protected_key].sum().sum())\n",
    "    protected_prop = protected_accepted/protected_total\n",
    "    if verbose:\n",
    "        print(protected_key.title() + ' proportion accepted: %.3f' % protected_prop)\n",
    "\n",
    "    # return adverse impact ratio\n",
    "    return ((protected_prop + eps)/(reference_prop + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_f1_frame(frame, y, yhat, res=0.01, air_reference=None, air_protected=None): \n",
    "    \n",
    "    \"\"\" Utility function for finding max. F1. \n",
    "        Coupled to get_confusion_matrix() and air(). \n",
    "        Assumes 1 is the marker for class membership.\n",
    "    \n",
    "        :param frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
    "        :param y: Known y values.\n",
    "        :param yhat: Model scores.\n",
    "        :param res: Resolution over which to search for max. F1, default 0.01.\n",
    "        :param air_reference: Reference group for AIR calculation, optional.\n",
    "        :param air_protected: Protected group for AIR calculation, optional.\n",
    "        :return: Pandas DataFrame of cutoffs to select from.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    do_air = all(v is not None for v in [air_reference, air_protected])\n",
    "    \n",
    "    # init frame to store f1 at different cutoffs\n",
    "    if do_air:\n",
    "        columns = ['cut', 'f1', 'acc', 'air']\n",
    "    else:\n",
    "        columns = ['cut', 'f1', 'acc']\n",
    "    f1_frame = pd.DataFrame(columns=['cut', 'f1', 'acc'])\n",
    "    \n",
    "    # copy known y and score values into a temporary frame\n",
    "    temp_df = frame[[y, yhat]].copy(deep=True)\n",
    "    \n",
    "    # find f1 at different cutoffs and store in acc_frame\n",
    "    for cut in np.arange(0, 1 + res, res):\n",
    "        temp_df['decision'] = np.where(temp_df.iloc[:, 1] > cut, 1, 0)\n",
    "        f1 = f1_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
    "        acc = accuracy_score(temp_df.iloc[:, 0], temp_df['decision'])\n",
    "        row_dict = {'cut': cut, 'f1': f1, 'acc': acc}\n",
    "        if do_air:\n",
    "            # conditionally calculate AIR  \n",
    "            cm_ref = get_confusion_matrix(frame, y, yhat, by=air_reference, level=1, cutoff=cut, verbose=False)\n",
    "            cm_pro = get_confusion_matrix(frame, y, yhat, by=air_protected, level=1, cutoff=cut, verbose=False)\n",
    "            air_ = air({air_reference: cm_ref, air_protected: cm_pro}, air_reference, air_protected, verbose=False)\n",
    "            row_dict['air'] = air_\n",
    "            \n",
    "        f1_frame = f1_frame.append(row_dict, ignore_index=True)\n",
    "            \n",
    "    del temp_df\n",
    "        \n",
    "    return f1_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebm_grid(train, valid, x_names, y_name, gs_params=None, n_models=None, early_stopping_rounds=None, seed=None,\n",
    "             air_reference=None, air_protected=None, air_cut=None):\n",
    "    \n",
    "    \"\"\" Performs a random grid search over n_models and gs_params.\n",
    "        Optionally considers random feature sets and AIR.\n",
    "        Coupled to get_confusion_matrix() and air(). \n",
    "\n",
    "    :param train: Training data as Pandas DataFrame.\n",
    "    :param valid: Validation data as Pandas DataFrame.\n",
    "    :param x_names: Names of input features.\n",
    "    :param y_name: Name of target feature.\n",
    "    :param gs_params: Dictionary of lists of potential EBM parameters over which to search.   \n",
    "    :param n_models: Number of random models to evaluate.\n",
    "    :param early_stopping_rounds: EBM early stopping rounds.\n",
    "    :param seed: Random seed for better interpretability.\n",
    "    :param air_reference: Reference group for AIR calculation, optional.\n",
    "    :param air_protected: Protected group for AIR calculation, optional.  \n",
    "    :param air_cut: Cutoff for AIR calculation, optional.\n",
    "    :return: Tuple: (Best EBM model, Pandas DataFrame of models to select from)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # init returned frame\n",
    "    do_air = all(v is not None for v in [air_reference, air_protected])\n",
    "    if do_air: \n",
    "        columns = list(gs_params.keys()) + ['features', 'auc', 'air']\n",
    "    else:\n",
    "        columns = list(gs_params.keys()) + ['auc']\n",
    "    ebm_grid_frame = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # cartesian product of gs_params\n",
    "    keys, values = zip(*gs_params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # preserve exact reproducibility for this function\n",
    "    np.random.seed(SEED) \n",
    "    \n",
    "    # select randomly from cartesian product space\n",
    "    selected_experiments = np.random.choice(len(experiments), n_models)\n",
    "\n",
    "    # set global params for seed, etc.\n",
    "    params = {'n_jobs': NTHREAD,\n",
    "              'early_stopping_rounds': early_stopping_rounds, \n",
    "              'random_state': SEED}\n",
    "\n",
    "    # init grid search loop\n",
    "    best_candidate = None\n",
    "    best_score = 0\n",
    "\n",
    "    # grid search loop\n",
    "    for i, exp in enumerate(selected_experiments):\n",
    "\n",
    "        params.update(experiments[exp])  # override global params with current grid run params\n",
    "\n",
    "        print('Grid search run %d/%d:' % (int(i + 1), int(n_models)))\n",
    "        print('Training with parameters:', params)\n",
    "        \n",
    "        # train \n",
    "        ebm = ExplainableBoostingClassifier(**params)\n",
    "        \n",
    "        # conditionally select random features \n",
    "        features = x_names\n",
    "        if do_air:\n",
    "            n_features = random.randrange(len(x_names)) + 1\n",
    "            features = random.sample(x_names, n_features)\n",
    "        candidate = ebm.fit(train[features], train[y_name]) \n",
    "\n",
    "        # calculate AUC\n",
    "        ebm_perf = ROC(ebm.predict_proba).explain_perf(valid[features], valid[y_name])\n",
    "        candidate_best_score = ebm_perf._internal_obj['overall']['auc']\n",
    "    \n",
    "        # compose values to add to ebm_grid_frame\n",
    "        row_dict = params.copy()\n",
    "        row_dict['auc'] = candidate_best_score\n",
    "        if do_air:\n",
    "            # collect random feature set\n",
    "            row_dict['features'] = features\n",
    "            # conditionally calculate AIR  \n",
    "            valid_phat = valid.copy(deep=True)\n",
    "            valid_phat['phat'] = candidate.predict_proba(valid[features])[:, 1]\n",
    "            cm_ref = get_confusion_matrix(valid_phat, y_name, 'phat', by=air_reference, level=1, cutoff=air_cut, verbose=False)\n",
    "            cm_pro = get_confusion_matrix(valid_phat, y_name, 'phat', by=air_protected, level=1, cutoff=air_cut, verbose=False)\n",
    "            air_ = air({air_reference: cm_ref, air_protected: cm_pro}, air_reference, air_protected, verbose=False)\n",
    "            row_dict['air'] = air_\n",
    "            del valid_phat\n",
    "\n",
    "        # append run to ebm_grid_frame\n",
    "        ebm_grid_frame = ebm_grid_frame.append(row_dict, ignore_index=True)\n",
    "    \n",
    "        # determine if current model is better than previous best\n",
    "        if candidate_best_score > best_score:\n",
    "            best_score = candidate_best_score\n",
    "            best_ebm = candidate\n",
    "            print('Grid search new best score discovered at iteration %d/%d: %.4f.' %\n",
    "                             (int(i + 1), int(n_models), candidate_best_score))\n",
    "\n",
    "        print('---------- ----------')\n",
    "        \n",
    "        del row_dict\n",
    "        del ebm\n",
    "            \n",
    "    return best_ebm, ebm_grid_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1623607175.4352942"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\benji\\\\OneDrive\\\\Desktop\\\\Responsible ML\\data\\hmda_train_preprocessed.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\benji\\\\OneDrive\\\\Desktop\\\\Responsible ML\\data\\hmda_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign basic modeling roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name = 'high_priced'\n",
    "x_names = ['term_360', 'conforming', 'debt_to_income_ratio_missing', 'loan_amount_std', 'loan_to_value_ratio_std', 'no_intro_rate_period_std',\n",
    "           'intro_rate_period_std', 'property_value_std', 'income_std', 'debt_to_income_ratio_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit interpretable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data rows = 112450, columns = 23\n",
      "Validation data rows = 47888, columns = 23\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED) # preserve exact reproducibility for this cell\n",
    "\n",
    "split_ratio = 0.7 # 70%/30% train/test split\n",
    "\n",
    "# execute split\n",
    "split = np.random.rand(len(data)) < split_ratio\n",
    "train = data[split]\n",
    "valid = data[~split]\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Validation data rows = %d, columns = %d' % (valid.shape[0], valid.shape[1]))\n",
    "\n",
    "# benchmark - Train data rows = 112253, columns = 23\n",
    "# benchmark - Validation data rows = 48085, columns = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainable Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search run 1/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "Grid search new best score discovered at iteration 1/10: 0.8182.\n",
      "---------- ----------\n",
      "Grid search run 2/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 3/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 4/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 5/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "Grid search new best score discovered at iteration 5/10: 0.8226.\n",
      "---------- ----------\n",
      "Grid search run 6/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "---------- ----------\n",
      "Grid search run 7/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 8/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 9/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 10/10:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "---------- ----------\n",
      "EBM training completed in 3087.94 s.\n"
     ]
    }
   ],
   "source": [
    "# dictionary of hyperparameter value lists for grid search\n",
    "gs_params = {'max_bins': [128, 256, 512],\n",
    "             'max_interaction_bins': [16, 32, 64],\n",
    "             'interactions': [5, 10, 15],\n",
    "             'outer_bags': [4, 8, 12], \n",
    "             'inner_bags': [0, 4],\n",
    "             'learning_rate': [0.001, 0.01, 0.05],\n",
    "             'validation_size': [0.1, 0.25, 0.5],\n",
    "             'min_samples_leaf': [1, 2, 5, 10],\n",
    "             'max_leaves': [1, 3, 5]}\n",
    "\n",
    "# start local timer\n",
    "ebm_tic = time.time()\n",
    "\n",
    "# EBM grid search\n",
    "best_ebm, ebm_grid_frame = ebm_grid(train, valid, x_names, y_name, gs_params=gs_params, n_models=10, \n",
    "                                    early_stopping_rounds=100, seed=SEED)\n",
    "\n",
    "# end local timer\n",
    "ebm_toc = time.time() - ebm_tic\n",
    "print('EBM training completed in %.2f s.' % (ebm_toc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic AUC Assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.8226.\n"
     ]
    }
   ],
   "source": [
    "best_ebm_perf = ROC(best_ebm.predict_proba).explain_perf(valid[x_names], valid[y_name])\n",
    "print('Validation AUC: %.4f.' % best_ebm_perf._internal_obj['overall']['auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score validation data with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>black</th>\n",
       "      <th>asian</th>\n",
       "      <th>white</th>\n",
       "      <th>amind</th>\n",
       "      <th>hipac</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>non_hispanic</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_to_income_ratio_missing</th>\n",
       "      <th>loan_amount_std</th>\n",
       "      <th>loan_to_value_ratio_std</th>\n",
       "      <th>no_intro_rate_period_std</th>\n",
       "      <th>intro_rate_period_std</th>\n",
       "      <th>property_value_std</th>\n",
       "      <th>income_std</th>\n",
       "      <th>debt_to_income_ratio_std</th>\n",
       "      <th>high_priced</th>\n",
       "      <th>phat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.074670</td>\n",
       "      <td>-1.150240</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>0.358276</td>\n",
       "      <td>-0.018133</td>\n",
       "      <td>-0.425131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.734255</td>\n",
       "      <td>0.552520</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.720941</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0</td>\n",
       "      <td>0.280186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.426448</td>\n",
       "      <td>0.355249</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.474263</td>\n",
       "      <td>-0.027488</td>\n",
       "      <td>-1.156406</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.866172</td>\n",
       "      <td>1.630935</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.875115</td>\n",
       "      <td>-0.041693</td>\n",
       "      <td>1.585876</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.558366</td>\n",
       "      <td>0.115325</td>\n",
       "      <td>0.244394</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.535932</td>\n",
       "      <td>-0.035803</td>\n",
       "      <td>-0.150903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  black  asian  white  amind  hipac  hispanic  non_hispanic  male  \\\n",
       "0       3    0.0    0.0    1.0    0.0    0.0       0.0           1.0   NaN   \n",
       "1       7    0.0    0.0    1.0    0.0    0.0       0.0           1.0   1.0   \n",
       "2      14    0.0    0.0    1.0    0.0    0.0       NaN           NaN   NaN   \n",
       "3      16    1.0    0.0    0.0    0.0    0.0       0.0           1.0   0.0   \n",
       "4      19    0.0    0.0    1.0    0.0    0.0       1.0           0.0   0.0   \n",
       "\n",
       "   female  ...  debt_to_income_ratio_missing  loan_amount_std  \\\n",
       "0     NaN  ...                             0        -0.074670   \n",
       "1     0.0  ...                             0        -0.734255   \n",
       "2     NaN  ...                             0        -0.426448   \n",
       "3     1.0  ...                             0        -0.866172   \n",
       "4     1.0  ...                             0        -0.558366   \n",
       "\n",
       "   loan_to_value_ratio_std  no_intro_rate_period_std  intro_rate_period_std  \\\n",
       "0                -1.150240                  0.244394              -0.215304   \n",
       "1                 0.552520                  0.244394              -0.215304   \n",
       "2                 0.355249                  0.244394              -0.215304   \n",
       "3                 1.630935                  0.244394              -0.215304   \n",
       "4                 0.115325                  0.244394              -0.215304   \n",
       "\n",
       "   property_value_std  income_std  debt_to_income_ratio_std  high_priced  \\\n",
       "0            0.358276   -0.018133                 -0.425131            0   \n",
       "1           -0.720941   -0.039614                  0.488963            0   \n",
       "2           -0.474263   -0.027488                 -1.156406            0   \n",
       "3           -0.875115   -0.041693                  1.585876            1   \n",
       "4           -0.535932   -0.035803                 -0.150903            0   \n",
       "\n",
       "       phat  \n",
       "0  0.003774  \n",
       "1  0.280186  \n",
       "2  0.108000  \n",
       "3  0.283334  \n",
       "4  0.038315  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ebm_phat = pd.DataFrame(best_ebm.predict_proba(valid[x_names])[:, 1], columns=['phat']) \n",
    "best_ebm_phat = pd.concat([valid.reset_index(drop=True), best_ebm_phat], axis=1)\n",
    "best_ebm_phat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Best Model (EBM) for Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cut        f1       acc\n",
      "0    0.00  0.179648  0.098689\n",
      "1    0.01  0.241264  0.388344\n",
      "2    0.02  0.271601  0.486677\n",
      "3    0.03  0.288822  0.535792\n",
      "4    0.04  0.303458  0.573025\n",
      "..    ...       ...       ...\n",
      "96   0.96  0.000000  0.901311\n",
      "97   0.97  0.000000  0.901311\n",
      "98   0.98  0.000000  0.901311\n",
      "99   0.99  0.000000  0.901311\n",
      "100  1.00  0.000000  0.901311\n",
      "\n",
      "[101 rows x 3 columns]\n",
      "\n",
      "Best EBM F1: 0.3743 achieved at cutoff: 0.18 with accuracy: 0.7942.\n"
     ]
    }
   ],
   "source": [
    "#Finding the optimal cutoff\n",
    "f1_frame = get_max_f1_frame(best_ebm_phat, y_name, 'phat')\n",
    "\n",
    "print(f1_frame)\n",
    "print()\n",
    "\n",
    "max_f1 = f1_frame['f1'].max()\n",
    "best_cut = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'cut'] #idxmax() returns the index of the maximum value\n",
    "acc = f1_frame.loc[int(f1_frame['f1'].idxmax()), 'acc']\n",
    "\n",
    "print('Best EBM F1: %.4f achieved at cutoff: %.2f with accuracy: %.4f.' % (max_f1, best_cut, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix by black=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1       468       951\n",
      "predicted: 0       255      1581\n",
      "\n",
      "Confusion matrix by asian=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1       111       218\n",
      "predicted: 0        58      2800\n",
      "\n",
      "Confusion matrix by white=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1      1995      5898\n",
      "predicted: 0      1251     25267\n",
      "\n",
      "Confusion matrix by male=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1      1018      3056\n",
      "predicted: 0       656     11059\n",
      "\n",
      "Confusion matrix by female=1\n",
      "             actual: 1 actual: 0\n",
      "predicted: 1       908      2089\n",
      "predicted: 0       432      6648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find confusion matrices for demographic groups\n",
    "demographic_group_names = ['black', 'asian', 'white', 'male', 'female']\n",
    "cm_dict = {}\n",
    "\n",
    "for name in demographic_group_names:\n",
    "    cm_dict[name] = get_confusion_matrix(best_ebm_phat, y_name, 'phat', by=name, level=1, cutoff=best_cut)\n",
    "    print(cm_dict[name])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White proportion accepted: 0.771\n",
      "Asian proportion accepted: 0.897\n",
      "Adverse impact ratio for Asian people vs. White people: 1.164\n"
     ]
    }
   ],
   "source": [
    "#Find AIR for Asian people\n",
    "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(cm_dict, 'white', 'asian'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White proportion accepted: 0.771\n",
      "Black proportion accepted: 0.564\n",
      "Adverse impact ratio for Black people vs. White people: 0.732\n"
     ]
    }
   ],
   "source": [
    "#Find AIR for Black people\n",
    "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(cm_dict, 'white', 'black'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male proportion accepted: 0.742\n",
      "Female proportion accepted: 0.703\n",
      "Adverse impact ratio for Females vs. Males: 0.947\n"
     ]
    }
   ],
   "source": [
    "#Find AIR for Males&Females  people\n",
    "print('Adverse impact ratio for Females vs. Males: %.3f' % air(cm_dict, 'male', 'female'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt remediation of discovered discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "      <th>air</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.362374</td>\n",
       "      <td>0.827452</td>\n",
       "      <td>0.800199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.351434</td>\n",
       "      <td>0.836138</td>\n",
       "      <td>0.823574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.344931</td>\n",
       "      <td>0.845097</td>\n",
       "      <td>0.846166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.333777</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.868480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.324209</td>\n",
       "      <td>0.861322</td>\n",
       "      <td>0.876977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cut        f1       acc       air\n",
       "21  0.21  0.362374  0.827452  0.800199\n",
       "22  0.22  0.351434  0.836138  0.823574\n",
       "23  0.23  0.344931  0.845097  0.846166\n",
       "24  0.24  0.333777  0.853700  0.868480\n",
       "25  0.25  0.324209  0.861322  0.876977"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_frame = get_max_f1_frame(best_ebm_phat, y_name, 'phat', air_reference='white', air_protected='black')\n",
    "# print highest quality cutoffs above four fifths rule cutoff\n",
    "f1_frame[f1_frame['air'] > 0.8].sort_values(by='f1', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adverse impact ratio for Asian people vs. White people: 1.097\n",
      "Adverse impact ratio for Black people vs. White people: 0.824\n",
      "Adverse impact ratio for Females vs. Males: 0.974\n"
     ]
    }
   ],
   "source": [
    "#Check that other groups are not adversely impacted by change\n",
    "# calculate new confusion matrics for each group\n",
    "rem_cm_dict = {}\n",
    "for name in demographic_group_names:\n",
    "    rem_cm_dict[name] = get_confusion_matrix(best_ebm_phat, y_name, 'phat', by=name, level=1, cutoff=0.22, verbose=False)\n",
    "\n",
    "# calculate AIR for each group\n",
    "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(rem_cm_dict, 'white', 'asian', verbose=False))\n",
    "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(rem_cm_dict, 'white', 'black', verbose=False))\n",
    "print('Adverse impact ratio for Females vs. Males: %.3f' % air(rem_cm_dict, 'male', 'female', verbose=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search run 1/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "Grid search new best score discovered at iteration 1/50: 0.8154.\n",
      "---------- ----------\n",
      "Grid search run 2/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 3/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "Grid search new best score discovered at iteration 3/50: 0.8155.\n",
      "---------- ----------\n",
      "Grid search run 4/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 5/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "---------- ----------\n",
      "Grid search run 6/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "Grid search new best score discovered at iteration 6/50: 0.8172.\n",
      "---------- ----------\n",
      "Grid search run 7/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 8/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.1, 'min_samples_leaf': 2, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 9/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 10/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 5}\n",
      "---------- ----------\n",
      "Grid search run 11/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 512, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.05, 'validation_size': 0.25, 'min_samples_leaf': 5, 'max_leaves': 3}\n",
      "Grid search new best score discovered at iteration 11/50: 0.8213.\n",
      "---------- ----------\n",
      "Grid search run 12/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 13/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 10, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 14/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 512, 'max_interaction_bins': 64, 'interactions': 10, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 15/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'learning_rate': 0.01, 'validation_size': 0.5, 'min_samples_leaf': 2, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 16/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 32, 'interactions': 15, 'outer_bags': 8, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.5, 'min_samples_leaf': 1, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 17/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 10, 'outer_bags': 12, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.1, 'min_samples_leaf': 1, 'max_leaves': 3}\n",
      "---------- ----------\n",
      "Grid search run 18/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 512, 'max_interaction_bins': 32, 'interactions': 5, 'outer_bags': 4, 'inner_bags': 0, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 10, 'max_leaves': 1}\n",
      "---------- ----------\n",
      "Grid search run 19/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 256, 'max_interaction_bins': 16, 'interactions': 15, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.05, 'validation_size': 0.1, 'min_samples_leaf': 5, 'max_leaves': 5}\n",
      "---------- ----------\n",
      "Grid search run 20/50:\n",
      "Training with parameters: {'n_jobs': 4, 'early_stopping_rounds': 100, 'random_state': 1988, 'max_bins': 128, 'max_interaction_bins': 16, 'interactions': 5, 'outer_bags': 12, 'inner_bags': 4, 'learning_rate': 0.001, 'validation_size': 0.25, 'min_samples_leaf': 2, 'max_leaves': 3}\n"
     ]
    }
   ],
   "source": [
    "#More sophisticated remdiation: Model selection via quality and fairness\n",
    "\n",
    "\n",
    "# start local timer\n",
    "ebm2_tic = time.time()\n",
    "\n",
    "# new grid search that also considers AIR and fairness\n",
    "best_ebm2, ebm_grid_frame = ebm_grid(train, best_ebm_phat, x_names, y_name, gs_params=gs_params, n_models=50, \n",
    "                                     early_stopping_rounds=100, seed=SEED, air_reference='white', air_protected='black', \n",
    "                                     air_cut=0.17)\n",
    "\n",
    "# end local timer\n",
    "ebm2_toc = time.time() - ebm2_tic\n",
    "print('EBM training completed in %.2f s.' % (ebm2_toc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display grid search results as table\n",
    "ebm_grid_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display grid search results as plot\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "_ = ebm_grid_frame.plot(kind='scatter', x='air', y='auc', title='AIR vs. AUC for EBMs', ax=ax)\n",
    "_ = ax.axvline(x=0.8, color='r', linestyle='--')\n",
    "_ = ax.set_ylim([0.4, 0.85])\n",
    "_ = ax.set_xlim([0.75, 1.05])\n",
    "_ = ax.set_xlabel('AIR')\n",
    "_ = ax.set_ylabel('AUC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain most accurate model above 0.8 AIR\n",
    "# extract new params dict from ebm_grid_frame\n",
    "rem_params = ebm_grid_frame.loc[ebm_grid_frame['air'] > 0.8].sort_values(by='auc', ascending=False).iloc[0, :].to_dict()\n",
    "\n",
    "# extract features from dict then delete from dict \n",
    "rem_x_names = rem_params['features']\n",
    "del rem_params['features']\n",
    "\n",
    "# record and delete other extraneous information\n",
    "print('Best AUC: %.4f above 0.8 AIR (%.4f).' % (rem_params['auc'], rem_params['air']))\n",
    "del rem_params['auc']\n",
    "del rem_params['air']\n",
    "\n",
    "# reset some parameters to integers\n",
    "rem_params['random_state'] = int(rem_params['random_state'])\n",
    "rem_params['n_jobs'] = int(rem_params['n_jobs'])\n",
    "\n",
    "# retrain\n",
    "rem_ebm = ExplainableBoostingClassifier(**rem_params)\n",
    "rem_ebm.fit(train[rem_x_names], train[y_name]) \n",
    "rem_ebm_perf = ROC(rem_ebm.predict_proba).explain_perf(valid[rem_x_names], valid[y_name])\n",
    "rem_auc = rem_ebm_perf._internal_obj['overall']['auc']\n",
    "print('Remediated EBM retrained with AUC: %.4f.' % rem_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frame with remediated EBM predictions\n",
    "best_ebm_phat2 = pd.DataFrame(rem_ebm.predict_proba(valid[rem_x_names])[:, 1], columns=['phat']) \n",
    "best_ebm_phat2 = pd.concat([valid.reset_index(drop=True), best_ebm_phat2], axis=1)\n",
    "\n",
    "# calculate new confusion matrices for each group\n",
    "rem_cm_dict2 = {}\n",
    "for name in demographic_group_names:\n",
    "    rem_cm_dict2[name] = get_confusion_matrix(best_ebm_phat2, y_name, 'phat', by=name, level=1, cutoff=0.17, verbose=False)\n",
    "\n",
    "# calculate AIR for each group\n",
    "print('Adverse impact ratio for Asian people vs. White people: %.3f' % air(rem_cm_dict2, 'white', 'asian', verbose=False))\n",
    "print('Adverse impact ratio for Black people vs. White people: %.3f' % air(rem_cm_dict2, 'white', 'black', verbose=False))\n",
    "print('Adverse impact ratio for Females vs. Males: %.3f' % air(rem_cm_dict2, 'male', 'female', verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End time\n",
    "toc = time.time() - tic\n",
    "print('All tasks completed in %.2f s.' % (toc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
